{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ed396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/01 13:58:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "print(\"loaded libraries\")\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"cohort identification\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789fd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_disp = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_patient_enc_demo_dispo.parquet\")\n",
    "demo_disp = demo_disp.withColumn('adm_date',f.to_date('adm_date','yyyy-MM-dd'))\n",
    "demo_disp = demo_disp.filter(((f.col('adm_date')>='2020-03-01') & \n",
    "                   (f.col('adm_date')<='2022-03-31')))\n",
    "demo_disp = demo_disp.filter(f.col('age_at_adm')>=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b5568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get worst FiO2\n",
    "resp_full = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_respiratory_support_09282023.csv')\n",
    "resp_full = resp_full.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "resp_full = resp_full.withColumn(\"fio2\",resp_full.fio2.cast('double'))\n",
    "resp_full = resp_full.withColumn(\"lpm\",resp_full.lpm.cast('double'))\n",
    "\n",
    "resp_full = resp_full.filter((((f.col('fio2')>=0.21) &\n",
    "                              (f.col('fio2')<=1)) |\n",
    "                              (f.col('fio2').isNull())))\n",
    "resp_full = resp_full.filter(~((f.col('device_name')=='NIPPV') &\n",
    "                              (f.col('fio2').isNull())))\n",
    "\n",
    "\n",
    "resp_full = resp_full.select('C19_HAR_ID', 'device_name', 'recorded_time', 'fio2', 'lpm')\n",
    "resp_full = resp_full.withColumn('meas_hour', f.hour(f.col('recorded_time')))\n",
    "resp_full = resp_full.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n",
    "\n",
    "fio2 = resp_full.select('C19_HAR_ID', 'device_name', 'meas_date', 'meas_hour', 'fio2', 'lpm')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\", \"device_name\", \"meas_date\", \"meas_hour\"]\n",
    "fio2 = fio2.groupBy(group_cols) \\\n",
    "            .agg((f.max('fio2').alias(\"fio2\")),\n",
    "                  (f.max('lpm').alias(\"lpm\")))\n",
    "\n",
    "fio2 = fio2.join(demo_disp, on='C19_HAR_ID', how='leftsemi')\n",
    "fio2 = fio2.withColumn('device_name', f.when(~f.col('device_name').rlike(r'NA'), f.col('device_name')))\n",
    "fio2 = fio2.withColumn('device_name', f.when(~f.col('device_name').rlike(r'NULL'), f.col('device_name')))\n",
    "\n",
    "#io2 = fio2.withColumn('fio2', f.when(~f.col('fio2').rlike(r'NA'), f.col('fio2')))\n",
    "#fio2 = fio2.withColumn('lpm', f.when(~f.col('lpm').rlike(r'NA'), f.col('lpm')))\n",
    "\n",
    "fio2 = fio2.withColumn('fio2_combined', f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2 IS NULL AND device_name == 'Room Air' THEN .21\n",
    "        WHEN fio2 IS NULL AND device_name == 'Nasal Cannula' THEN ( 0.24 + (0.04 * lpm) )\n",
    "        WHEN fio2 IS NOT NULL THEN fio2\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8559fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fio2_hours = fio2.select(\"C19_HAR_ID\",\"meas_date\").distinct()\n",
    "fio2_hours = fio2_hours.groupBy('C19_HAR_ID').agg((f.min('meas_date').alias(\"first_date\")),\n",
    "                                               (f.max('meas_date').alias(\"last_date\")))\n",
    "\n",
    "fio2_hours = fio2_hours.withColumn('first_date',f.to_timestamp('first_date','yyyy-MM-dd'))\n",
    "fio2_hours = fio2_hours.withColumn('last_date',f.to_timestamp('last_date','yyyy-MM-dd'))\n",
    "\n",
    "\n",
    "fio2_hours = fio2_hours.withColumn('txnDt', f.explode(f.expr('sequence(first_date, last_date, interval 1 hour)')))\n",
    "fio2_hours = fio2_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\n",
    "fio2_hours = fio2_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\n",
    "fio2_hours = fio2_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\n",
    "fio2_hours = fio2_hours.join(fio2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt')\n",
    "\n",
    "fio2_hours_2 = fio2_hours.withColumn('device_filled', f.when((f.col('device_name').isNotNull()), f.col('device_name')))\n",
    "fio2_hours_2 = fio2_hours_2.withColumn('device_filled', f.coalesce(f.col('device_name'), f.last('device_name', True).over(Window.partitionBy('C19_HAR_ID').orderBy('txnDt')), f.lit('NULL')))\n",
    "fio2_hours_2 = fio2_hours_2.withColumn('device_filled', f.when(~f.col('device_filled').rlike(r'NULL'), f.col('device_filled')))\n",
    "\n",
    "fio2_hours_2 = fio2_hours_2.withColumn(\"fio2_combined\",fio2_hours_2.fio2_combined.cast('double'))\n",
    "\n",
    "fio2_hours_2 = fio2_hours_2.withColumn('fio2_filled', f.when((f.col('fio2_combined').isNotNull()), f.col('fio2_combined')))\n",
    "fio2_hours_2 = fio2_hours_2.withColumn('fio2_filled', f.coalesce(f.col('fio2_combined'), f.last('fio2_combined', True).over(Window.partitionBy('C19_HAR_ID', 'device_filled').orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "fio2_filled = fio2_hours_2.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour', 'device_filled','fio2_filled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a5504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now need PaO2\n",
    "labs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs_10312023.parquet\")\n",
    "\n",
    "\n",
    "### Cleaning up values/columns\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time','lab_name', 'lab_value')\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_name'), \"[\\ufeff]\", \"\").alias('lab_name')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_value', *select_expr)\n",
    "\n",
    "labs = labs.filter(f.col(\"lab_name\")==\"pao2\")\n",
    "\n",
    "labs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[\\ufeff]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[<]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[>]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "labs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\n",
    "labs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\n",
    "labs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n",
    "labs = labs.withColumn(\"lab_value_num\",labs.lab_value.cast('double'))\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "labs = labs.groupBy(group_cols) \\\n",
    "           .pivot(\"lab_name\") \\\n",
    "           .agg(f.min('lab_value_num').alias(\"min\"))\n",
    "labs = labs.join(demo_disp, on='C19_HAR_ID', how='leftsemi')\n",
    "\n",
    "labs_hours = labs.select(\"C19_HAR_ID\",\"meas_date\").distinct()\n",
    "labs_hours = labs_hours.groupBy('C19_HAR_ID').agg((f.min('meas_date').alias(\"first_date\")),\n",
    "                                               (f.max('meas_date').alias(\"last_date\")))\n",
    "\n",
    "labs_hours = labs_hours.withColumn('first_date',f.to_timestamp('first_date','yyyy-MM-dd'))\n",
    "labs_hours = labs_hours.withColumn('last_date',f.to_timestamp('last_date','yyyy-MM-dd'))\n",
    "\n",
    "\n",
    "labs_hours = labs_hours.withColumn('txnDt', f.explode(f.expr('sequence(first_date, last_date, interval 1 hour)')))\n",
    "labs_hours = labs_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\n",
    "labs_hours = labs_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\n",
    "labs_hours = labs_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour')\n",
    "\n",
    "labs_hours = labs_hours.join(labs, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt')\n",
    "\n",
    "\n",
    "labs_hours_2 = labs_hours.withColumn('last_measure', f.when(f.col('pao2').isNotNull(), f.col('txnDt')))\n",
    "labs_hours_2 = labs_hours_2.withColumn('last_measure', f.coalesce(f.col('last_measure'), f.last('last_measure', True).over(Window.partitionBy('C19_HAR_ID').orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "labs_hours_2 = labs_hours_2.withColumn('last_measure',f.to_timestamp('last_measure','yyyy-MM-dd HH:mm:ss'))\n",
    "labs_hours_2 = labs_hours_2.withColumn('txnDt',f.to_timestamp('txnDt','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "labs_hours_2 = labs_hours_2.withColumn(\"hour_diff\", (f.col(\"txnDt\").cast(\"long\")-f.col(\"last_measure\").cast(\"long\"))/(60*60))\n",
    "labs_hours_2 = labs_hours_2.filter((f.col('hour_diff')>=0)&(f.col('hour_diff')<=3))\n",
    "\n",
    "labs_hours_2 = labs_hours_2.withColumn(\"pao2_num\",labs_hours_2.pao2.cast('double'))\n",
    "labs_hours_2 = labs_hours_2.filter(f.col('pao2_num')>0)\n",
    "\n",
    "labs_hours_2 = labs_hours_2.withColumn('pao2_filled', f.when(f.col('pao2_num').isNotNull(), f.col('pao2_num')))\n",
    "labs_hours_2 = labs_hours_2.withColumn('pao2_filled', f.coalesce(f.col('pao2_num'), f.last('pao2_num', True).over(Window.partitionBy('C19_HAR_ID', 'last_measure').orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "pao2_filled = labs_hours_2.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour', 'pao2_filled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c31200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now need spO2\n",
    "vitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals_10242023.parquet\")\n",
    "vitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "vitals = vitals.select('C19_HAR_ID', 'measured_time','vital_name', 'vital_value')\n",
    "\n",
    "vitals = vitals.filter(f.col(\"vital_name\")==\"spO2\")\n",
    "\n",
    "vitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\n",
    "vitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\n",
    "vitals = vitals.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'vital_name', 'vital_value')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "vitals = vitals.groupBy(group_cols) \\\n",
    "           .pivot(\"vital_name\") \\\n",
    "           .agg(f.min('vital_value').alias(\"min\"))\n",
    "vitals = vitals.join(demo_disp, on='C19_HAR_ID', how='leftsemi')\n",
    "\n",
    "vitals_hours = vitals.select(\"C19_HAR_ID\",\"meas_date\").distinct()\n",
    "vitals_hours = vitals_hours.groupBy('C19_HAR_ID').agg((f.min('meas_date').alias(\"first_date\")),\n",
    "                                               (f.max('meas_date').alias(\"last_date\")))\n",
    "\n",
    "vitals_hours = vitals_hours.withColumn('first_date',f.to_timestamp('first_date','yyyy-MM-dd'))\n",
    "vitals_hours = vitals_hours.withColumn('last_date',f.to_timestamp('last_date','yyyy-MM-dd'))\n",
    "\n",
    "\n",
    "vitals_hours = vitals_hours.withColumn('txnDt', f.explode(f.expr('sequence(first_date, last_date, interval 1 hour)')))\n",
    "vitals_hours = vitals_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\n",
    "vitals_hours = vitals_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\n",
    "vitals_hours = vitals_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour')\n",
    "\n",
    "vitals_hours = vitals_hours.join(vitals, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt')\n",
    "\n",
    "\n",
    "vitals_hours_2 = vitals_hours.withColumn('last_measure', f.when(f.col('spO2').isNotNull(), f.col('txnDt')))\n",
    "vitals_hours_2 = vitals_hours_2.withColumn('last_measure', f.coalesce(f.col('last_measure'), f.last('last_measure', True).over(Window.partitionBy('C19_HAR_ID').orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "vitals_hours_2 = vitals_hours_2.withColumn('last_measure',f.to_timestamp('last_measure','yyyy-MM-dd HH:mm:ss'))\n",
    "vitals_hours_2 = vitals_hours_2.withColumn('txnDt',f.to_timestamp('txnDt','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "vitals_hours_2 = vitals_hours_2.withColumn(\"spO2_num\",vitals_hours_2.spO2.cast('double'))\n",
    "vitals_hours_2 = vitals_hours_2.filter(f.col('spO2_num')>60)\n",
    "vitals_hours_2 = vitals_hours_2.filter(f.col('spO2_num')<=100)\n",
    "\n",
    "\n",
    "\n",
    "vitals_hours_2 = vitals_hours_2.withColumn('spO2_filled', f.when(f.col('spO2_num').isNotNull(), f.col('spO2_num')))\n",
    "vitals_hours_2 = vitals_hours_2.withColumn('spO2_filled', f.coalesce(f.col('spO2_num'), f.last('spO2_num', True).over(Window.partitionBy('C19_HAR_ID', 'last_measure').orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "spO2_filled = vitals_hours_2.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour', 'spO2_filled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c53b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FiO2, PaO2, spO2 to get FiO2/PaO2\n",
    "\n",
    "fio2_filled = fio2_filled.repartition('C19_HAR_ID')\n",
    "pao2_filled = pao2_filled.repartition('C19_HAR_ID')\n",
    "spO2_filled = spO2_filled.repartition('C19_HAR_ID')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"txnDt\",\"meas_date\", \"meas_hour\"]\n",
    "df = fio2_filled.join(pao2_filled, on=group_cols, how='full')\n",
    "df = df.join(spO2_filled, on=group_cols, how='full')\n",
    "\n",
    "df = df.withColumn(\"fio2_filled\",df.fio2_filled.cast('double'))\n",
    "df = df.withColumn(\"pao2_filled\",df.pao2_filled.cast('double'))\n",
    "df = df.withColumn(\"spO2_filled\",df.spO2_filled.cast('double'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430890c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first time on oxygen support & P/F <200\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "df = df.withColumn(\"p_f\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2_filled IS NOT NULL AND pao2_filled IS NOT NULL THEN ( pao2_filled / fio2_filled )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "df = df.withColumn(\"s_f\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2_filled IS NOT NULL AND spO2_filled IS NOT NULL THEN ( spO2_filled / fio2_filled )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335bce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 14:01:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:01:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:02:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.distinct()\n",
    "df.write.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597b00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((((f.col(\"p_f\")<200))|\n",
    "                (f.col(\"s_f\")<200)))\n",
    "df = df.filter(f.col(\"device_filled\")!=\"NULL\")\n",
    "df = df.filter(f.col(\"device_filled\")!=\"Room Air\")\n",
    "df = df.filter(f.col(\"device_filled\")!=\"Vent\")\n",
    "df = df.filter(f.col(\"device_filled\")!=\"NIPPV\")\n",
    "df = df.filter(f.col(\"device_filled\").isNotNull())\n",
    "\n",
    "\n",
    "df = df.select(\"C19_HAR_ID\", \"txnDt\", \"meas_date\", \"meas_hour\", \"device_filled\",\"pao2_filled\",\"fio2_filled\",\n",
    "              \"spO2_filled\")\n",
    "\n",
    "w1 = Window.partitionBy(\"C19_HAR_ID\").orderBy('txnDt')\n",
    "\n",
    "df_first_with_time = df.withColumn(\"row\",f.row_number().over(w1)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")\n",
    "\n",
    "df_first_with_time = df_first_with_time.select(\"C19_HAR_ID\", \"txnDt\").withColumnRenamed(\"txnDt\", \"recorded_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d861da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get just invasive or non-invasive mechanical ventilation\n",
    "vent = resp_full.filter(((f.col('device_name')=='Vent') | \n",
    "                   (f.col('device_name')=='NIPPV')))\n",
    "\n",
    "# minimum time by person\n",
    "\n",
    "w3 = Window.partitionBy(\"C19_HAR_ID\").orderBy('recorded_time')\n",
    "\n",
    "vent_first = vent.withColumn(\"row\",f.row_number().over(w3)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8443ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with oxygen support and P/F < 200 group, get first time meeting criteria\n",
    "vent_first = vent_first.repartition('C19_HAR_ID')\n",
    "df_first_with_time = df_first_with_time.repartition('C19_HAR_ID')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"recorded_time\"]\n",
    "df = vent_first.join(df_first_with_time, on=group_cols, how='full')\n",
    "\n",
    "resp_support = df.groupBy(\"C19_HAR_ID\").agg(f.min(\"recorded_time\").alias(\"resp_life_support_start\")).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca7b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now pressors\n",
    "df_meds = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_meds_admin_conti.csv')\n",
    "df_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_time','yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4cd7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressors = df_meds.filter(((f.col('med_name')=='phenylephrine') | \n",
    "                       (f.col('med_name')=='epinephrine') | \n",
    "                       (f.col('med_name')=='vasopressin') | \n",
    "                       (f.col('med_name')=='dopamine') |\n",
    "                       (f.col('med_name')=='dobutamine') |\n",
    "                       (f.col('med_name')=='norepinephrine') |\n",
    "                       (f.col('med_name')=='angiotensin') |\n",
    "                       (f.col('med_name')=='isoproterenol')))\n",
    "pressors = pressors.select(\"C19_HAR_ID\", \"admin_time\")\n",
    "\n",
    "pressors = pressors.groupBy(\"C19_HAR_ID\").agg(f.min(\"admin_time\").alias(\"pressor_life_support_start\"))\n",
    "pressors = pressors.join(demo_disp, on='C19_HAR_ID', how='leftsemi').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c0d150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 14:04:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/11/01 14:04:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "df = pressors.join(resp_support, on='C19_HAR_ID', how='full')\n",
    "df = df.withColumn(\"life_support_start\", f.least(f.col('pressor_life_support_start'),\n",
    "                                                 f.col('resp_life_support_start')))\n",
    "df = df.select('C19_HAR_ID', 'life_support_start')\n",
    "df = df.join(demo_disp, on='C19_HAR_ID', how='inner').orderBy('adm_date').distinct()\n",
    "df.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252d917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
