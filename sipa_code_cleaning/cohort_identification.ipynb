{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96b70f31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4889fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/24 15:00:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/24 15:00:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "print(\"loaded libraries\")\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"cohort identification\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be8524c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get worst FiO2\n",
    "resp_full = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_respiratory_support_09282023.csv')\n",
    "resp_full = resp_full.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "resp_full = resp_full.filter(((f.col('recorded_time')>='2020-03-01 00:00:00') & \n",
    "                   (f.col('recorded_time')<='2022-03-31 11:59:59')))\n",
    "\n",
    "resp_full = resp_full.select('C19_HAR_ID', 'device_name', 'recorded_time', 'fio2')\n",
    "resp_full = resp_full.withColumn('meas_hour', f.hour(f.col('recorded_time')))\n",
    "resp_full = resp_full.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n",
    "\n",
    "fio2 = resp_full.filter(f.col(\"fio2\")!=\"NA\")\n",
    "fio2 = fio2.filter(f.col(\"device_name\")!=\"NA\")\n",
    "fio2 = fio2.filter(f.col(\"device_name\")!=\"Room Air\")\n",
    "\n",
    "\n",
    "fio2 = fio2.select('C19_HAR_ID', 'device_name', 'meas_date', 'meas_hour', 'fio2')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\", \"device_name\", \"meas_date\", \"meas_hour\"]\n",
    "fio2 = fio2.groupBy(group_cols) \\\n",
    "            .agg(f.max('fio2').alias(\"fio2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4acace55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now need PaO2\n",
    "labs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\n",
    "\n",
    "\n",
    "### Cleaning up values/columns\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time','lab_name', 'lab_value')\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_name'), \"[\\ufeff]\", \"\").alias('lab_name')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_value', *select_expr)\n",
    "\n",
    "labs = labs.filter(f.col(\"lab_name\")==\"pao2\")\n",
    "\n",
    "labs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n",
    "labs = labs.filter(((f.col('lab_result_time')>='2020-03-01 00:00:00') & \n",
    "                   (f.col('lab_result_time')<='2022-03-31 11:59:59')))\n",
    "\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[\\ufeff]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[<]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "select_expr = [f.regexp_replace(f.col('lab_value'), \"[>]\", \"\").alias('lab_value')]\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time', 'lab_name', *select_expr)\n",
    "\n",
    "labs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\n",
    "labs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\n",
    "labs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n",
    "labs = labs.withColumn(\"lab_value_num\",labs.lab_value.cast('double'))\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "labs = labs.groupBy(group_cols) \\\n",
    "           .pivot(\"lab_name\") \\\n",
    "           .agg(f.min('lab_value_num').alias(\"min\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eea0c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FiO2 and PaO2 to get FiO2/PaO2\n",
    "\n",
    "fio2 = fio2.repartition('C19_HAR_ID')\n",
    "labs = labs.repartition('C19_HAR_ID')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "df = fio2.join(labs, on=group_cols, how='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23f1464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first time on oxygen support & P/F <200\n",
    "\n",
    "pf_df = df.withColumn(\"p_f\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2 IS NOT NULL AND pao2 IS NOT NULL THEN ( pao2 / fio2 )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "pf_df = pf_df.filter(f.col(\"p_f\").isNotNull())\n",
    "df = pf_df.filter(f.col(\"p_f\")<200)\n",
    "df = df.filter(f.col(\"device_name\")!=\"Vent\")\n",
    "df = df.filter(f.col(\"device_name\")!=\"NIPPV\")\n",
    "\n",
    "\n",
    "df = df.select(\"C19_HAR_ID\",\"meas_date\", \"meas_hour\", \"device_name\", \"p_f\",\"pao2\",\"fio2\")\n",
    "\n",
    "order_cols = [\"meas_date\", \"meas_hour\"]\n",
    "w1 = Window.partitionBy(\"C19_HAR_ID\").orderBy(order_cols)\n",
    "\n",
    "df_first = df.withColumn(\"row\",f.row_number().over(w1)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"device_name\",\"meas_date\", \"meas_hour\",\"fio2\"]\n",
    "df_first_with_time = df_first.join(resp_full, on=group_cols, how=\"left\")\n",
    "df_first_with_time = df_first_with_time.select(\"C19_HAR_ID\",\"recorded_time\", \"meas_date\", \n",
    "                                               \"meas_hour\",\"device_name\",\"p_f\",\"pao2\",\"fio2\")\n",
    "\n",
    "w2 = Window.partitionBy(\"C19_HAR_ID\").orderBy(\"recorded_time\")\n",
    "\n",
    "df_first_with_time = df_first_with_time.withColumn(\"row\",f.row_number().over(w2)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9d861da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get just invasive or non-invasive mechanical ventilation\n",
    "vent = resp_full.filter(((f.col('device_name')=='Vent') | \n",
    "                   (f.col('device_name')=='NIPPV')))\n",
    "\n",
    "# minimum time by person\n",
    "\n",
    "w3 = Window.partitionBy(\"C19_HAR_ID\").orderBy('recorded_time')\n",
    "\n",
    "vent_first = vent.withColumn(\"row\",f.row_number().over(w3)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef5b2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with oxygen support and P/F < 200 group, get first time meeting criteria\n",
    "vent_first = vent_first.repartition('C19_HAR_ID')\n",
    "df_first_with_time = df_first_with_time.repartition('C19_HAR_ID')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"recorded_time\", \"meas_date\", \"meas_hour\",\"device_name\"]\n",
    "df = vent_first.join(df_first_with_time, on=group_cols, how='full')\n",
    "\n",
    "resp_support = df.groupBy(\"C19_HAR_ID\").agg(f.min(\"recorded_time\").alias(\"resp_life_support_start\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aca7b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now pressors\n",
    "df_meds = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_meds_admin_conti.csv')\n",
    "df_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_time','yyyy-MM-dd HH:mm:ss'))\n",
    "df_meds = df_meds.filter(((f.col('admin_time')>='2020-03-01 00:00:00') & \n",
    "                   (f.col('admin_time')<='2022-03-31 11:59:59')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e4cd7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressors = df_meds.filter(((f.col('med_name')=='phenylephrine') | \n",
    "                       (f.col('med_name')=='epinephrine') | \n",
    "                       (f.col('med_name')=='vasopressin') | \n",
    "                       (f.col('med_name')=='dopamine') |\n",
    "                       (f.col('med_name')=='dobutamine') |\n",
    "                       (f.col('med_name')=='norepinephrine') |\n",
    "                       (f.col('med_name')=='angiotensin') |\n",
    "                       (f.col('med_name')=='isoproterenol')))\n",
    "pressors = pressors.select(\"C19_HAR_ID\", \"admin_time\")\n",
    "\n",
    "pressors = pressors.groupBy(\"C19_HAR_ID\").agg(f.min(\"admin_time\").alias(\"pressor_life_support_start\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8c0d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = pressors.join(resp_support, on='C19_HAR_ID', how='full')\n",
    "df = df.withColumn(\"life_support_start\", f.least(f.col('pressor_life_support_start'),\n",
    "                                                 f.col('resp_life_support_start')))\n",
    "df = df.select('C19_HAR_ID', 'life_support_start')\n",
    "df.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a406115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
